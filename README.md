# AIRFLOW-WEATHER-FETCHING-ETL

This is a complete apache airflow etl pipeline to fetch, transform and load data from openweathermapapi


- **Python Version**: `3.10.12`
- **Airflow version**: `3.0.2`
- **Run this project into a env**: execute in the **terminal** `echo 'export AIRFLOW_HOME="$(pwd)/airflow"' >> .venv/bin/activate` in `/AIRFLOW-WEATHER-FETCHING-ETL`.

- **Package used** are in **`requirement.txt`**

### PROJECT STRUCTURE

`This project should be run in a virtual ENVIRONMENT`

- Required packages are in `requirement.txt`
- the actual airflow project is in `/airflow` with the subfolders:
  - `/dags`: containing the dag `weather_etl.py`
  - `/dags/scripts`: containing the ETL script for daily data.
  - `/dags/historical-scripts`: containing the ETL script for the historical data from `open-meteo` ( only needed to run once).
  - **Two folders** `/data` and `/historical-data` who are generated by the ETL scripts in `/scripts` and `/historical-scripts`
    - `/data/raw` & `/historical-data/raw`  contain subfolders labeled by **extraction_date** and contains the csv files of each city named **{city_name}.csv**
    - `/data/star_schema` contain the star schema dimension `dim_city`, `dim_date`, `dim_meteo`and the fact table `fact_weather`
    - `/data/processed` contained the data from `/data/raw` & `/historical-data/raw` processed and merged into `meteo_global.csv`  
  - `airflow.cfg` is the configuration file of the airflow apache

### CONFIGURATION AND NEEDED ENVIRONMENT VARIABLE

#### CONFIGURATION

After cloning the repository

- Go into `/AIRFLOW-WEATHER-FETCHING-ETL`
- Run `echo 'export AIRFLOW_HOME="$(pwd)/airflow"' >> .venv/bin/activate`
- activate the venv
- Install the needed packages from requirement.txt
- migrate the db, create and admin user and reset+migrate the db again  
- In `airflow.cfg` change the following variable value
  - `dags_folders` into `{your_path}/AIRFLOW-WEATHER-FETCHING-ETL/airflow/dags`
  - `plugins_folder` into `/home/radiant_wizard/Project/AIRFLOW-WEATHER-FETCHING-ETL/airflow/plugins`
  - `sql_alchemy_conn` into `sqlite:////{your_path}/AIRFLOW-WEATHER-FETCHING-ETL/airflow/airflow.db`
  - `base_log_folder` into `{your_path}/AIRFLOW-WEATHER-FETCHING-ETL/airflow/logs`
  - `dag_processor_child_process_log_directory` into `{your_path}/AIRFLOW-WEATHER-FETCHING-ETL/airflow/logs/dag_processor`
  - `config_file` into `/home/radiant_wizard/Project/AIRFLOW-WEATHER-FETCHING-ETL/airflow/webserver_config.py`
- NB: the example dags are set to false in the cfg 

#### ENVIRONMENT VARIABLE NEEDED

- Into a `.env` file add the following variable:
  - **GOOGLE_SERVICE_ACCOUNT_JSON**: the content of your **credential.json** file from google cloud console that is encoded in base64 to keep its formatting.
  - **DRIVE_FOLDER_ID**: the id of your google drive folder to keep the data from `/data`

- For the airflow environment variable:
  - **API_KEY**: the api key from open weather.
  - **DRIVE_FOLDER_ID**: the id of your google drive folder.
  - **GOOGLE_SERVICE_ACCOUNT_JSON**: the content of your **credential.json** file from google cloud console that is needed to be set in the terminal with `airflow variables set GOOGLE_SERVICE_ACCOUNT_JSON "$(cat {your_path_to_the_credentials}/credentials.json)"` to keep its format.

## Scripts summary

- the scripts in `/historical-scripts` are meant to be run once to extract raw data from open meteo (**extract.py**), **clean** and **merge** them into the **meteo_global.csv**.
  - The **load.py** is meant to load the raw data into a drive folders but it require a good connection and take time due to the heavy number of files. (**18^day_count**).
  - The date range can be changed by changing the value of "start_date" and "end_date" in `params = {...}` in extract.py
- the scripts in `/scripts` are like the scripts of `historical-data` but:
  - Have **transform.py** that transform the data from meteo_global.csv into dimension and fact table
  - **load.py** load the data from `/data` into the drive folder and keep their paths
